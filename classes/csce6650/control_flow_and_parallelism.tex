\documentclass[12pt,twoside,letterpaper]{article}
\usepackage{graphicx}
\setlength\topmargin{0in}
\setlength\headheight{0in}
\setlength\headsep{0in}
\setlength\textheight{9.0in}
\setlength\textwidth{6.5in}
\setlength\oddsidemargin{0in}
\setlength\evensidemargin{0in}
\setlength{\parskip}{12pt}
\pagestyle{empty}
\raggedright
\begin{document}
Cameron L. Palmer \\
University of North Texas \\
Fall 2008

\title{Control Flow Inhibiting Parallelism}

\begin{abstract}
The age of the multi-processor desktop computer has been with us for a few years now. As we ran out of ways to ramp up processor clock, companies have put multiple processors on a die to increase performance. For the typical desktop running a handful of applications and an operating system, this direction has improved real-world performance for users considerably. Of course this improvement in performance does not come from a proliferation of multi-threaded programming, or a magical compiler technique, it relies simply on the ability of the operating system to interleave the programs execution on your two or more processors. Applications largely remain single threaded, unable to take advantage of the multi-threading on their own. To take advantage of larger numbers of processors we will need to extract more parallelism from imperative programs.
\end{abstract}

\section*{Thesis Proposal}
I propose to investigate the limits of thread-level parallelism (TLP) in imperative code. Therehave been studies of the limits of instruction-level parallelism (ILP) available in imperative code. The amount of thread-level parallelism will give us an upper bound on the amount of parallelism we can extract from existing programs in the SPEC CPU benchmark suite, and then map to the modern multi-core processors available on most desktops.

\section*{Background Information}
\subsection*{Basic Processor Technology}
The modern processor, or von Neumann architecture, is broken into several distinct stages. These stages within the context of the MIPS processor are Instruction Fetch (IF), Instruction Decode (ID), Execute (EX), Memory Access (MA), and Write Back (WB). An instruction is taken from the queue of instructions to be executed, decoded and the program counter (PC) incremented, executed through a manipulation in the Arithmetic Logic Unit (ALU), possibly transfer to or from main memory, and finally write results into a register. The ALU is one of the components that make up every CPU and these components the perform the work of the stages are referred to as functional units. In this simplistic version of computer processor architecture, every instruction completes in one-cycle and the cycle is the length of time required to complete the longest instruction. 

The next logical step is use these stage boundaries to divide up instructions into multiple cycles, by pipelining the processor. At any given stage the processor can be using all of the functional units in the processor simultaneously. For example one instruction could be decoding while a second instruction could be in the execute phase, and a third in write back. This system allows instructions to be completed every cycle in the case where there are no dependences between instructions. An illustration of the simple pipelined architecture can be found in Figure \ref{mips_pipeline}.

We can further subdivide the stages of the processor, resulting in a superpipelined architecture, and if we add multiple sets of parallel functional units within the processor and allow our processor to issue multiple instructions simultaneously, we call this superscalar. The PowerPC 970 processor is an example of a superscalar design that has four ALUs, two Floating Point Units (FPUs), and two Single Instruction Mulitple Data (SIMD) units. The combination of superscalar and superpipelining can get the processor to complete multiple instructions per cycle.

Realistically, the instructions aren't going to be dependence free, and they may not be in an ideal order, so we get stalls where nothing is happening in a particular part of the processor for several cycles. So we need instruction reordering to help hide latencies and keep the functional units busy. Instruction reordering has been in computing hardware since the CDC 6600 (Scoreboarding) and the IBM System 360 Model 91 (Tomasulo's Algorithm). This allows the processor to reorder instructions on-the-fly to maximize the throughput of instructions and hopefully get us close to one instruction completing per cycle. Instruction reordering can also be done by the compiler, and in architectures like VLIW this compile-time instruction ordering is critical.

\begin{figure}
	\begin{center}
		\includegraphics[scale=.15]{pipeline_MIPS.eps}
	\end{center}
	
	\caption{The simple pipelined processor - REPLACE ME!}
	\label{mips_pipeline}
\end{figure}

\subsection*{Memory}
The processor will need memory, and the most common method of interfacing memory and the processor is through some sort of cache. The speed of memory has not kept pace with processor speed increases so cache helps hide the slow speed of main memory.  

\begin{quote}
Although CPUs are a thousand times faster, memory speed has increased only by a factor of ten or so. Back in the 1980s, reading a bit from main memory took a few hundred nanoseconds, which was also the time needed to execute a single instruction in a CPU. The memory and the processor cycles were well matched. Today, a processor could execute a hundred instructions in the time it takes to get data from memory.
\end{quote}

So we have developed very good cache predictors to try and have the correct instructions or data in cache to avoid the significant delay. Of course cache introduces its own problems, energy consumption, and lack of predictable latency which makes cache less useful in real-time embedded designs. Cache memory uses a significant amount of space on the CPU die and has caused some processors like Cell to eliminate cache altogether, or explore new approaches like scratchpad memory.

\section*{Diminishing Returns}
The doubling of transistors that can fit on a die every 18 months has had the nice advantage of packing more transistors in the same die space. This means the amount of power consumption has remained constant because the transistors got smaller. In fact processor has grown by .7 with each generation which is a reduction in size. The power requirements have been reduced too, but not at the same rate. Since smaller transistor leak power constantly, increasing speed by packing more transitors onto a die doesn't seem to be the best way forward.

Intel, AMD, IBM and others have started putting multiple CPUs on the die to sidestep the issue of increasing processor speed.

\section*{Multi-core processors to the rescue, or we have multiple cores, now what?}

We start off talking about super-scaling and pipelining and how that ran into trouble. Then what the industry did, namely multi-core to the rescue. Introduce why this solution pushes the problem back to software.


\section*{Extracting parallelism}
\section*{What is limiting instruction-level parallelism?}
Control flow
\section*{Why we aren't multi-threading everything}

ILP is limited but why? We investigate this and the suspicion that control flow plays a role. We should touch on the difficulties and limitations of current multi-threading methodologies, and how I believe a compiler technique to 


\section*{Exotic solutions we are ignoring}
\section*{Other programming paradigms}
Functional programming languages
\section*{Alternative architectures}
Dataflow

\section*{Code hoisting in SSA form}

Novel Architectures for Parallelism
Scheduled Dataflow (SDF)
As much time as our lab has spent discussing SDF I think it is important to discuss my research in relation to SDF. SDF is an architecture that decouples memory accesses from execution. By having an execution processor and a memory access processor, the ISA for SDF supports the explicit creation of threads and sets them to execute as their data values arrive. This enables the processor to execute in-order code without waiting on data values to arrive from memory, and the next thread can begin executing because write back would move the thread back to the memory access processor. This approach solves some of the issues with original dataflow architectures that operated in this fashion on a per instruction basis, which provided too fine grained threads. This architecture suffers from the same problem all attempts at multi-threading, the basic block is too small to achieve any reasonable amounts of parallelism. In fact the very problems SDF solves become major liabilities because it generates more instruction overhead for thread creation, and the movement of threads into and out of queues makes control flow latencies worse.

Even in the paper presented to the IEEE, the authors concede that the processor cannot compete with a superscalar processor and instead add multiple processors to their design to make it perform better, arguing that when those cores aren’t being used they could be switched off. This seems a tacit admission that this architecture, more than others, suffers from the lack of a technique of extracting parallelism from programs, and that without that, further discussion about the merits of this architecture are moot. I believe, that before we start worrying about decoupling memory accesses, and explicitly creating frames, we need  more code to stuff into frames. 

Multi-threaded Programming Paradigms
Functional
The Library Approach

\section*{Trace Scheduling}

\section*{Instruction-Level Parallelism}
Instruction-Level Parallelism (ILP) is the amount of parallelism that can be found in instructions through reordering. The first commercial machines that could exploit ILP were CDC’s 6600 and IBM’s System 360 Model 91, both in the 1960s, using Scoreboarding and Tomasulo’s Algorithm respectively. Today the idea of multiple issue and out-of-order execution is found in most processors on the market, and is the cornerstone of superscalar, superpipelined desktop processors on the market today. To make processors perform faster we want to finish instructions every cycle, and to do that we must issue multiple instructions each cycle as operands become available, and keep all the functional units as busy as possible.

One of the best attempts at determining the amount of ILP available in the average program was attempted by David Wall. Joseph Fisher pointed in [1993 History, Overview, Perspective] that it is possible that a new technique could discover more parallelism than Wall does in this paper, just like more ILP was available in 1993 than the pessimistic 1.86 found in basic blocks in [1970 Tjaden and Flynn], but this paper does the best job of performing an aggressive limit study of ILP.

Wall identifies early in the paper what limits parallelism. One problem is dependencies between instructions. True dependencies, where the outcome of one instruction feeds into the next instruction cannot be removed. The true dependence represents the fundamental ordering inherent in tasks, just like I must obtain food before consuming it. Other tasks aren't as rigidly ordered but an artificial register dependency can be created because of poorly planning during register assignment. This poor planning might be the reuse of a register that prevents an independent task from executing in parallel and can be addressed with register renaming. A similar but more difficult version of this problem can happen with memory operations. It can be difficult to determine at compile time if two operations are using the same memory location. To use Wall’s example:

\begin{quote}
r1 := 0[r9]
4[r16] := r3
\end{quote}

We cannot tell if this load and store is to the same location or different locations. We must use alias analysis to try to clear this up, but in practice we often err on the side of safety and enforce strict ordering.

Wall, unlike many early investigations of ILP, tries to look beyond basic blocks. The number of instructions Wall found between jumps (a basic block) is small, often averaging less than 6. According to [Patterson and Hennessy 4th edition] MIPS has between 3-6 instructions in its basic blocks. These small basic blocks require us to stop and wait for the branch evaluation to occur so that we can choose the right path. In order to hide these latencies we can use branch prediction, and potentially speculation. This introduces the problem of speculative fanout since jumps are so frequent you can still be evaluating a previous branch when you encounter the next. Loop unrolling is another way to enlarge basic blocks to get more parallelism. When we unroll a loop numerous times, we remove branches and have more to work instructions to move. Wall also mentions software pipelining, and trace scheduling to further enlarge blocks.

The experiments in this paper try loosening hardware constraints, adding branch prediction, loop unrolling, and still Wall doesn’t find much more parallelism. Average parallelism was around 5.








ILP goes back at least as far as the CDC 6600 and IBM’s System 360 Model 91, allowing computers to issue instructions as soon as their operands are available, potentially out-of-order. ILP technology is commonplace in processors today. Superscalar and superpipelined processors use this sort of parallelism to speed up execution to issue multiple instructions simultaneously and keeping instructions in flight.

Today’s processors look at the upcoming instructions and reorder and issue them as soon as possible or to hide latencies in instructions. If there are multiple functional units the processor can issue independent instructions in parallel. In the case of Very Long Instruction Word (VLIW) architectures the instructions were packed together at compile time to take advantage of the multiple functional units in the processor.

This paper attempts a limit study of ILP, and this forms a foundation for several other important papers. In the Tjaden and Flynn (1970) paper very little ILP was found within basic block, only a factor of 2 to 3. However, as this paper points out, one must look across basic block boundaries. The paper wants to establish an upper bound on the amount of parallelism available and roughly establishes two roads to more parallelism: increasing the parallelism within blocks, and crossing block boundaries.

The limitation 
\end{document}
